{"cells":[{"cell_type":"markdown","source":["# List of all Notebooks and Resources for this Project\n","(https://drive.google.com/file/d/1Z8vPNZAcivWOxeh3UKFfeARbQCMkQ_NR/view?usp=sharing)\n"],"metadata":{"id":"NKa9zOCUTzIx"}},{"cell_type":"markdown","metadata":{"id":"u4vp1t3pRRbl"},"source":["* Cleaning:\n","    * Application Train/Test: https://drive.google.com/file/d/17PZtY-xD-6AbF_9B0CkhxnD25WAUhnzP/view?usp=sharing\n","    * Bureau Balance: https://drive.google.com/file/d/17CWrXSq0UD59yT0LgF_VXaqc-nkVcdFM/view?usp=sharing\n","    * Bureau: https://drive.google.com/file/d/16oY7SJ5Gup31-BsuykAEf6UxrKtMOjy-/view?usp=sharing\n","    * Credit Card: https://drive.google.com/file/d/17Xt2BNZ_AbtZDq3u5fUCz20u-27kmxfp/view?usp=sharing\n","    * Installments Payments: https://drive.google.com/file/d/17QxdLEpcFDgRFi9W28VSJgVFDU6cPLi9/view?usp=sharing\n","    * POS CASH: https://drive.google.com/file/d/16n5gXaxqB59kyMqAB5thSLDFwrxoqg0u/view?usp=sharing\n","    * Previous Application: https://drive.google.com/file/d/16Pl8cB-basjgk0aGTTi7NnlYXGJX7yje/view?usp=sharing\n","\n","* EDA:\n","    * Application Train: https://drive.google.com/file/d/1WXiFmq0IVBg7ALqsjeDDq1DGYdf1tCjy/view?usp=sharing\n","    * Bureau: https://drive.google.com/file/d/16Yrojb1GLrqCbb9b1ADqxFHryxDxxbyw/view?usp=sharing\n","    * Previous Application: https://drive.google.com/file/d/15y6WRC9rUUmH9cvzN8pTXuQIEaVX7tTl/view?usp=sharing\n","\n","* Model:\n","    * Feature Selection: https://drive.google.com/file/d/17AJWJKRkDIhD8Xe89T-Vg8lw6Nwqooyq/view?usp=sharing\n","    * Model Application Train Only: https://drive.google.com/file/d/16NjK4XB-TDXoDYRgso8mEtj8FrmZtGhy/view?usp=sharing\n","    * Model All Tables: https://drive.google.com/file/d/17FsG6U-pZuVAhapyLvpVZ0RxtmDRb-vb/view?usp=sharing\n","\n","* Python-file with functions used: https://drive.google.com/file/d/1IsRcGuolR4Hnu6bGe44GcS_0UQPFM59h/view?usp=sharing\n"]},{"cell_type":"markdown","source":["# Processing Plan"],"metadata":{"id":"V-JmaceoTbq4"}},{"cell_type":"markdown","source":["\n","Plan for data wrangling:\n","\n","- POS CASH, installment, and credit cards belong the the previous applications table and show the monthly payment.\n","- Aggregate them per previous ID (mean, latest, fraction of time on specific status) and merge with previous application.\n","- Bureau Balance belongs to bureau table and protocolls the monthly payments. Treat in same manner as above.\n","- Create new features in bureau, current, and previous applications (e.g. annuity/credit, annuity/income, etc.)\n","- Aggregate Bureau and previous applications with their adjacent/merged subtables on current ID\n","- Merge both with the applications table and create features based on all tables (e.g. (total annuity)/income)\n","\n","EDA:  \n","\n","- on bureau, current, and previous applications, merged with target from applications table to study correlations/dependencies.\n","\n","Model:\n","\n","- 2 extremes: no past credit information (model on app table only), and extensiv past credit information (model on all tables)\n","- RFECV with LightGBM on both subsets\n","- study and select model for each subset with corresponding RFE-selected features and deploy\n"],"metadata":{"id":"9FseaTHOTWJp"}},{"cell_type":"markdown","source":["# Objective"],"metadata":{"id":"8CdVTv-1Hxbj"}},{"cell_type":"markdown","source":["You and your friend came up with a brilliant startup idea - provide risk evaluation as a service for retail banks. As with most successful startup teams, both of you have your specialty. Your friend is responsible for sales and operations, while you are responsible for everything product-related, from planning to data analysis to building the solution. You have quickly identified that machine learning will be an essential part of your offering because you believe that the models can capture statistical patterns in the defaults on bank loans. You decide to start your investigation by downloading this dataset from Home Credit Group. You are not yet sure, what is the most crucial problem for your potential clients, so you had a meeting with your friend to discuss how your proof-of-concept (POC) product should look like. After a lot of arguing, you both agreed to create a number of different models so that you have a robust and diversified offering when you get your first meeting with the potential clients. You are eager to investigate the dataset and see what you can predict, so you propose that you come up with interesting features to analyze and predict - this way, you'll focus on building a solid offering, and she can work on getting meetings with the banks.\n","Objectives for this Part\n","\n"," *   Practice translating business requirements into data science tasks.\n"," *   Practice performing EDA.\n"," *   Practice applying statistical inference procedures.\n"," *   Practice using machine learning to solve business problems.\n"," *   Practice deploying multiple machine learning models.\n","\n","Requirements\n","\n"," *   Download the data from [here](https://storage.googleapis.com/341-home-credit-default/home-credit-default-risk.zip) and the data description from [here](https://storage.googleapis.com/341-home-credit-default/Home%20Credit%20Default%20Risk.pdf).\n"," *   Create a plan for your investigation, analysis, and POC building. This should include your assumptions, overall objectives, and objectives for each step in your plan. You are not expected to have a plan for the whole project but instead have a clear understanding of what you'll try to achieve in the next step and build the plan one step at a time.\n"," *   Perform exploratory data analysis. This should include creating statistical summaries and charts, testing for anomalies, checking for correlations and other relations between variables, and other EDA elements.\n"," *   Perform statistical inference. This should include defining the target population, forming multiple statistical hypotheses and constructing confidence intervals, setting the significance levels, conducting z or t-tests for these hypotheses.\n"," *   Use machine learning models to predict the target variables based on your proposed plan. You should use hyperparameter tuning, model ensembling, the analysis of model selection, and other methods. The decision of where to use and not to use these techniques is up to you; however, they should be aligned with your team's objectives.\n"," *   Deploy these machine learning models to Google Cloud Platform. You are free to choose any deployment option you wish as long as it can be called an HTTP request.\n"," *   Provide clear explanations in your notebook. Your explanations should inform the reader what you are trying to achieve, what results you got, and what these results mean.\n"," *   Provide suggestions about how your analysis and models can be improved.\n"],"metadata":{"id":"w1kulAF6HAZP"}},{"cell_type":"markdown","source":["# Remarks"],"metadata":{"id":"HZSBVexTK2TU"}},{"cell_type":"markdown","source":["- RFECV (with  LightGBM as estimator) returned sometimes very few features despite the huge amount of features for all tables combined. Experiments with features from LightGBM importance or RFECV selection yield similar results compared to the initial feature selection from RFECV with a simple decision tree (for the sake of speed).\n","\n","\n","- painful lesson at deployment: LightGBM model created in the sklearn framework cannot be loaded - loading will call LightGBM-native routines that cannot read the sklearn-native storage pattern of the modules pickle file. need to explore the onnx-module to translate\n","\n"],"metadata":{"id":"vcG4EuVeK33b"}}],"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}